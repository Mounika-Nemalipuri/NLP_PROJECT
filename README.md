 Text summarization is a critical task in Natural Language Processing (NLP) that aims to condense long pieces of text into concise and meaningful summaries. 
 This project compares the performance of three models: LSTM with Attention, T5, and BART, for abstractive summarization. 
 Experiments were conducted on the CNN-DailyMail and AG-News datasets to evaluate the models under constrained computational settings.
 The study reveals that transformer-based pretrained models significantly outperform LSTM-based architectures, providing coherent and contextually relevant summaries even with limited training data.
